{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZQD8NqPhKyBP"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score # Please note that this is the only sklearn function that can be utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YV1MHt_VTg9f"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a1vkTOD6K5Nj"
   },
   "outputs": [],
   "source": [
    "# Load the train/val/test dataset\n",
    "\n",
    "df_train = pd.DataFrame(pd.read_csv(\"./PR_HW3_Train.csv\"))\n",
    "df_val   = pd.DataFrame(pd.read_csv(\"./PR_HW3_Val.csv\"))\n",
    "df_test  = pd.DataFrame(pd.read_csv(\"./PR_HW3_Test.csv\"))\n",
    "\n",
    "X_train = df_train[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
    "y_train = df_train[\"Target\"].to_numpy()\n",
    "\n",
    "X_val = df_val[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
    "y_val = df_val[\"Target\"].to_numpy()\n",
    "\n",
    "X_test = df_test[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
    "y_test = df_test[\"Target\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MJcktFIuK78Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 7)\n",
      "(800,)\n",
      "(800, 7)\n",
      "(800,)\n",
      "(800, 7)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa3hnJ9sTkvh"
   },
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5e_nviB8LAK8"
   },
   "outputs": [],
   "source": [
    "def gini(sequence):\n",
    "    _, counts = np.unique(sequence, return_counts=True)\n",
    "    return 1 - np.sum((counts / np.sum(counts))**2)\n",
    "\n",
    "def entropy(sequence):\n",
    "    _, counts = np.unique(sequence, return_counts=True)\n",
    "    return -np.sum((counts / np.sum(counts)) * np.log2(counts / np.sum(counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_HJA_108LF_G"
   },
   "outputs": [],
   "source": [
    "class Tree():\n",
    "    \"\"\"\n",
    "        You can add/change any variables/methods to meet your need.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_features=None, max_depth=None):\n",
    "        \"\"\"\n",
    "            You can add/change any variables/methods to meet your need.\n",
    "        \"\"\"\n",
    "\n",
    "        if criterion == 'gini':\n",
    "            self.criterion = gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.criterion = entropy\n",
    "        \n",
    "        if max_depth is None:\n",
    "            self.max_depth = 1e9\n",
    "        else:\n",
    "            self.max_depth = max_depth\n",
    "        \n",
    "        self.max_features = max_features\n",
    "        self.root = None\n",
    "        \n",
    "        self.importance = {}\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if depth == self.max_depth or len(y) == 0:\n",
    "            return Tree(value=np.bincount(y).argmax())\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        feature_idxs = np.random.choice(n_features, self.max_features, replace=False)\n",
    "\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_score = np.inf\n",
    "\n",
    "        for feature_idx in feature_idxs:\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, feature_idx], y)))\n",
    "            for i in range(1, n_samples):\n",
    "                if classes[i-1] != classes[i]:\n",
    "                    threshold = (thresholds[i-1] + thresholds[i]) / 2\n",
    "                    y_left = y[X[:, feature_idx] <= threshold]\n",
    "                    y_right = y[X[:, feature_idx] > threshold]\n",
    "                    score = (len(y_left) * self.criterion(y_left) + len(y_right) * self.criterion(y_right)) / n_samples\n",
    "                    if score < best_score:\n",
    "                        best_feature = feature_idx\n",
    "                        best_threshold = threshold\n",
    "                        best_score = score\n",
    "\n",
    "        if best_feature is None:\n",
    "            return Tree(value=np.bincount(y).argmax())\n",
    "\n",
    "        X_left, y_left = X[X[:, best_feature] <= best_threshold], y[X[:, best_feature] <= best_threshold]\n",
    "        X_right, y_right = X[X[:, best_feature] > best_threshold], y[X[:, best_feature] > best_threshold]\n",
    "\n",
    "        left = self._build_tree(X_left, y_left, depth+1)\n",
    "        right = self._build_tree(X_right, y_right, depth+1)\n",
    "\n",
    "        return Tree(best_feature, best_threshold, left=left, right=right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        y = y.astype(int)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if self.max_features is None:\n",
    "            self.max_features = n_features\n",
    "\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "         # Calculate feature importance\n",
    "        self.countImportance(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        y_pred = []\n",
    "        for sample in X:\n",
    "            node = self.tree_\n",
    "            while node.left:\n",
    "                if sample[node.feature] <= node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            y_pred.append(node.value)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def countImportance(self, X, y):\n",
    "        total_samples = len(y)\n",
    "\n",
    "        # Calculate impurity before the split\n",
    "        if self.criterion == gini:\n",
    "            parent_impurity = gini(y)\n",
    "        elif self.criterion == entropy:\n",
    "            parent_impurity = entropy(y)\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            # Count how many samples there are for each feature value\n",
    "            feature_counts = {}\n",
    "            for i in range(len(X)):\n",
    "                if X[i][feature] not in feature_counts:\n",
    "                    feature_counts[X[i][feature]] = 0\n",
    "                feature_counts[X[i][feature]] += 1\n",
    "\n",
    "            # Calculate the weighted impurity of the split on this feature\n",
    "            weighted_impurity = 0\n",
    "            for feature_value, count in feature_counts.items():\n",
    "                indices = np.where(X[:, feature] == feature_value)[0]\n",
    "                feature_impurity = self.criterion(y[indices])\n",
    "                weighted_impurity += (count / total_samples) * feature_impurity\n",
    "\n",
    "            # Calculate the feature importance as the reduction in impurity\n",
    "            self.importance[feature] = parent_impurity - weighted_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "BE8wu0MGN_H-"
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators=10, max_features=None, boostrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.trees.append(DecisionTree(criterion=self.criterion, max_depth=self.max_depth, max_features=self.max_features))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.boostrap:\n",
    "            bootstrap_size = len(X)\n",
    "        else:\n",
    "            bootstrap_size = None\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            bootstrap_indices = [random.randint(0, len(X)-1) for i in range(bootstrap_size)]\n",
    "            bootstrap_X = X[bootstrap_indices]\n",
    "            bootstrap_y = y[bootstrap_indices]\n",
    "            tree.fit(bootstrap_X, bootstrap_y)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            tree_predictions = []\n",
    "            for tree in self.trees:\n",
    "                tree_predictions.append(tree.predict([x])[0])\n",
    "            predictions.append(max(set(tree_predictions), key=tree_predictions.count))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPUsaCh2T9Fs"
   },
   "source": [
    "# Questions for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zSB-Uqp4OaaX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+' '+' '+' '+' '+' '-']: entropy = 0.6500224216483541\n",
      "['+' '+' '+' '-' '-' '-']: entropy = 1.0\n",
      "['+' '-' '-' '-' '-' '-']: entropy = 0.6500224216483541\n",
      "\n",
      "['+' '+' '+' '+' '+' '-']: gini index = 0.2777777777777777\n",
      "['+' '+' '+' '-' '-' '-']: gini index = 0.5\n",
      "['+' '-' '-' '-' '-' '-']: gini index = 0.2777777777777777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Q1\n",
    "ex1 = np.array([\"+\", \"+\", \"+\", \"+\", \"+\", \"-\"])\n",
    "ex2 = np.array([\"+\", \"+\", \"+\", \"-\", \"-\", \"-\"])\n",
    "ex3 = np.array([\"+\" ,\"-\", \"-\", \"-\", \"-\", \"-\"])\n",
    "\n",
    "print(\"{}: entropy = {}\\n{}: entropy = {}\\n{}: entropy = {}\\n\".format(ex1, entropy(ex1), ex2, entropy(ex2), ex3, entropy(ex3)))\n",
    "print(\"{}: gini index = {}\\n{}: gini index = {}\\n{}: gini index = {}\\n\".format(ex1, gini(ex1), ex2, gini(ex2), ex3, gini(ex3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "G_t9N9fnOdon"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2-1 max_depth=3:  0.73125\n"
     ]
    }
   ],
   "source": [
    "# For Q2-1, validation accuracy should be higher than or equal to 0.73\n",
    "\n",
    "np.random.seed(0) # You may adjust the seed number in all the cells\n",
    "\n",
    "dt_depth3 = DecisionTree(criterion='gini', max_features=None, max_depth=3)\n",
    "dt_depth3.fit(X_train, y_train)\n",
    "\n",
    "acc = accuracy_score(y_val, dt_depth3.predict(X_val))\n",
    "\n",
    "print(\"Q2-1 max_depth=3: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "T0HcgzMdjHRP"
   },
   "outputs": [],
   "source": [
    "\"\"\" Do Not Modify Below \"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as SK_DecisionTreeClassifier\n",
    "\n",
    "sk_dt = SK_DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
    "sk_dt.fit(X_train, y_train)\n",
    "sk_acc = accuracy_score(y_val, sk_dt.predict(X_val))\n",
    "\n",
    "assert round(acc, 3) == round(sk_acc, 3), \"Because the Decision Tree without any trick has a fixed answer, your accuracy should be the same as sklearn, otherwise your implementation might have some problems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "SjCPMr-eQ7jn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2-2 max_depth=10:  0.865\n"
     ]
    }
   ],
   "source": [
    "# For Q2-2, validation accuracy should be higher than or equal to 0.85\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "dt_depth10 = DecisionTree(criterion='gini', max_features=None, max_depth=10)\n",
    "dt_depth10.fit(X_train, y_train)\n",
    "\n",
    "print(\"Q2-2 max_depth=10: \", accuracy_score(y_val, dt_depth10.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iTbxGPrbO2jT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3-1 criterion='gini':  0.73125\n"
     ]
    }
   ],
   "source": [
    "# For Q3-1, validation accuracy should be higher than or equal to 0.73\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "dt_gini = DecisionTree(criterion='gini', max_features=None, max_depth=3)\n",
    "dt_gini.fit(X_train, y_train)\n",
    "\n",
    "print(\"Q3-1 criterion='gini': \", accuracy_score(y_val, dt_gini.predict(X_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1XG7eAKUQ-YU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3-2 criterion='entropy':  0.77\n"
     ]
    }
   ],
   "source": [
    "# For Q3-2, validation accuracy should be higher than or equal to 0.77\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "dt_entropy = DecisionTree(criterion='entropy', max_features=None, max_depth=3)\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "print(\"Q3-2 criterion='entropy': \", accuracy_score(y_val, dt_entropy.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For Q4\n",
    "\n",
    "# Use simply counting to get the feature importance: dt_depth10.importance\n",
    "\n",
    "labelList=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']\n",
    "\n",
    "fi = dt_depth10.importance\n",
    "sorted_idx = np.argsort(fi)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, fi[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(labelList)[sorted_idx])\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg97qz_xUGfP"
   },
   "source": [
    "# Questions for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "SlrdIW1ERJ8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6-1 n_estimators=10:  0.8875\n"
     ]
    }
   ],
   "source": [
    "# For Q6-1, validation accuracy should be higher than or equal to 0.88\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "rf_estimators10 = RandomForest(n_estimators=10, max_features=np.sqrt(X_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
    "rf_estimators10.fit(X_train, y_train)\n",
    "\n",
    "print(\"Q6-1 n_estimators=10: \", accuracy_score(y_val, rf_estimators10.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "4qcLuIkbRUfM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6-1 n_estimators=50:  0.8925\n"
     ]
    }
   ],
   "source": [
    "# For Q6-2, validation accuracy should be higher than or equal to 0.89\n",
    "\n",
    "np.random.seed(50)\n",
    "\n",
    "rf_estimators50 = RandomForest(n_estimators=50, max_features=np.sqrt(X_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
    "rf_estimators50.fit(X_train, y_train)\n",
    "\n",
    "print(\"Q6-1 n_estimators=50: \", accuracy_score(y_val, rf_estimators50.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "n-DbniYhRYmM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q7-1 max_features='sqrt':  0.89125\n"
     ]
    }
   ],
   "source": [
    "# For Q7-1, validation accuracy should be higher than or equal to 0.88\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "rf_maxfeature_sqrt = RandomForest(n_estimators=10, max_features=np.sqrt(X_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
    "rf_maxfeature_sqrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Q7-1 max_features='sqrt': \", accuracy_score(y_val,  rf_maxfeature_sqrt.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "PF9yufSaRffn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q7-1 max_features='All':  0.87375\n"
     ]
    }
   ],
   "source": [
    "# For Q7-2, validation accuracy should be higher than or equal to 0.86\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "rf_maxfeature_none = RandomForest(n_estimators=10, max_features=None, boostrap=True, criterion='gini', max_depth=None)\n",
    "rf_maxfeature_none.fit(X_train, y_train)\n",
    "\n",
    "print(\"Q7-1 max_features='All': \", accuracy_score(y_val, rf_maxfeature_none.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjopdAZqUKbF"
   },
   "source": [
    "# Train your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train/val/test dataset\n",
    "\n",
    "df_train = pd.DataFrame(pd.read_csv(\"./PR_HW3_Train.csv\"))\n",
    "df_val   = pd.DataFrame(pd.read_csv(\"./PR_HW3_Val.csv\"))\n",
    "df_test  = pd.DataFrame(pd.read_csv(\"./PR_HW3_Test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
    "y_train = df_train[\"Target\"].to_numpy()\n",
    "\n",
    "X_val = df_val[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
    "y_val = df_val[\"Target\"].to_numpy()\n",
    "\n",
    "X_test = df_test[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
    "y_test = df_test[\"Target\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "Va4L29gfUPO8"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_features=None, max_depth=None):\n",
    "        \"\"\"\n",
    "            You can add/change any variables/methods to meet your need.\n",
    "        \"\"\"\n",
    "\n",
    "        if criterion == 'gini':\n",
    "            self.criterion = gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.criterion = entropy\n",
    "        \n",
    "        if max_depth is None:\n",
    "            self.max_depth = 1e9\n",
    "        else:\n",
    "            self.max_depth = max_depth\n",
    "        \n",
    "        self.max_features = max_features\n",
    "        self.root = None\n",
    "        \n",
    "        self.importance = {}\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Check if stopping criteria are met\n",
    "        if depth >= self.max_depth or len(np.unique(y)) == 1 or n_samples == 0:\n",
    "            return Node(value=np.mean(y))\n",
    "\n",
    "        # Choose the best feature to split on\n",
    "        best_feature, best_threshold = self._get_best_split(X, y)\n",
    "\n",
    "        # Split the data\n",
    "        left_indices = np.where(X[:, best_feature] <= best_threshold)[0]\n",
    "        right_indices = np.where(X[:, best_feature] > best_threshold)[0]\n",
    "\n",
    "        # Check if splitting the data resulted in an empty left or right branch\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return Node(value=np.mean(y))\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left = self._build_tree(X[left_indices, :], y[left_indices], depth=depth+1)\n",
    "        right = self._build_tree(X[right_indices, :], y[right_indices], depth=depth+1)\n",
    "\n",
    "        # Create a new node with the best feature and threshold\n",
    "        return Node(feature=best_feature, threshold=best_threshold, left=left, right=right)\n",
    "    \n",
    "    def _get_best_split(self, X, y):\n",
    "        best_feature, best_threshold, best_impurity = None, None, 1e9\n",
    "\n",
    "        # Randomly select a subset of features to consider for splitting\n",
    "        features = np.random.choice(X.shape[1], size=self.max_features, replace=False)\n",
    "\n",
    "        # Loop through each feature and each possible threshold value to find the best split\n",
    "        for feature in features:\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                # Split the data on this feature and threshold\n",
    "                left_indices = np.where(X[:, feature] <= threshold)[0]\n",
    "                right_indices = np.where(X[:, feature] > threshold)[0]\n",
    "\n",
    "                # Calculate the impurity of the split using the chosen criterion\n",
    "                if self.criterion == gini:\n",
    "                    impurity = (len(left_indices) / len(y)) * gini(y[left_indices]) \\\n",
    "                               + (len(right_indices) / len(y)) * gini(y[right_indices])\n",
    "                elif self.criterion == entropy:\n",
    "                    impurity = (len(left_indices) / len(y)) * entropy(y[left_indices]) \\\n",
    "                               + (len(right_indices) / len(y)) * entropy(y[right_indices])\n",
    "\n",
    "                # If this is the best split seen so far, update the best feature and threshold\n",
    "                if impurity < best_impurity:\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_impurity = impurity\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.max_features is None:\n",
    "            self.max_features = n_features\n",
    "\n",
    "        # Build the tree\n",
    "        self.root = self._build_tree(X, y)\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        self.countImportance(X, y)\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def countImportance(self, X, y):\n",
    "        total_samples = len(y)\n",
    "\n",
    "        # Calculate impurity before the split\n",
    "        if self.criterion == gini:\n",
    "            parent_impurity = gini(y)\n",
    "        elif self.criterion == entropy:\n",
    "            parent_impurity = entropy(y)\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            # Count how many samples there are for each feature value\n",
    "            feature_counts = {}\n",
    "            for i in range(len(X)):\n",
    "                if X[i][feature] not in feature_counts:\n",
    "                    feature_counts[X[i][feature]] = 0\n",
    "                feature_counts[X[i][feature]] += 1\n",
    "\n",
    "            # Calculate the weighted impurity of the split on this feature\n",
    "            weighted_impurity = 0\n",
    "            for feature_value, count in feature_counts.items():\n",
    "                indices = np.where(X[:, feature] == feature_value)[0]\n",
    "                feature_impurity = self.criterion(y[indices])\n",
    "                weighted_impurity += (count / total_samples) * feature_impurity\n",
    "\n",
    "            # Calculate the feature importance as the reduction in impurity\n",
    "            self.importance[feature] = parent_impurity - weighted_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators=10, max_features=None, boostrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.trees.append(DecisionTree(criterion=self.criterion, max_depth=self.max_depth, max_features=self.max_features))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.boostrap:\n",
    "            bootstrap_size = len(X)\n",
    "        else:\n",
    "            bootstrap_size = None\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            bootstrap_indices = [random.randint(0, len(X)-1) for i in range(bootstrap_size)]\n",
    "            bootstrap_X = X[bootstrap_indices]\n",
    "            bootstrap_y = y[bootstrap_indices]\n",
    "            tree.fit(bootstrap_X, bootstrap_y)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            tree_predictions = []\n",
    "            for tree in self.trees:\n",
    "                tree_predictions.append(tree.predict([x])[0])\n",
    "            predictions.append(max(set(tree_predictions), key=tree_predictions.count))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of my model for validation data:  0.89875\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "your_model = RandomForest(n_estimators=20, max_features=np.sqrt(X_train.shape[1]), boostrap=True, criterion='entropy', max_depth=None)\n",
    "your_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy of my model for validation data: \", accuracy_score(y_val, your_model.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "5cmxQjK3Rja9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pred shape:  (800,)\n"
     ]
    }
   ],
   "source": [
    "test_pred = your_model.predict(X_test)\n",
    "\n",
    "print(\"test_pred shape: \", np.array(test_pred).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "XCaZ4yFuR34B"
   },
   "outputs": [],
   "source": [
    "# output csv\n",
    "df_test = pd.DataFrame(pd.read_csv(\"./PR_HW3_Test.csv\"))\n",
    "df_test[\"Target\"] = test_pred\n",
    "df_test.to_csv(\"311553009_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YV1MHt_VTg9f"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
